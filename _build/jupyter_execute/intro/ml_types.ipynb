{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0515e99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of ML\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1600/1*Iz7bCLrPTImnBDOOEyE3LA.png\n",
    ":alt: supervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Supervised learning is a popular category of machine learning algorithms that involves training a model on labeled data to make predictions or decisions. In this approach, the algorithm learns from a given set of input-output pairs and uses this knowledge to predict the output for new, unseen inputs. The goal is to find a mapping function that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2042ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now put it more mathematically. Denote\n",
    "\n",
    "* training **dataset** $\\mathcal D = \\{(\\boldsymbol x_i, y_i)\\}_{i=1}^N$;\n",
    "* **features** $\\boldsymbol x \\in \\mathcal X$ (usually $\\mathcal X = \\mathbb R^D$);\n",
    "* **targets** (**labels**) $y_i \\in \\mathcal Y$.\n",
    "\n",
    "The goal of the supervised learning is to find a mapping $f\\colon \\mathcal X \\to \\mathcal Y$ which would minimize the **cost** (**loss**) **function** \n",
    "\n",
    "$$\n",
    "\\mathcal L = \\frac 1N \\sum\\limits_{i=1}^N \\ell(y_i, f(\\boldsymbol x_i)).\n",
    "$$\n",
    "\n",
    "Note that the loss $\\ell(y_i, f(\\boldsymbol x_i))$ is calculated separately on each training object $(\\boldsymbol x_i, y_i)$, and then averaged over the whole training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e65fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f888d2",
   "metadata": {},
   "source": [
    "The mapping $f_{\\boldsymbol \\theta}\\colon \\mathcal X \\to \\mathcal Y$ is usually taken from some parametric family \n",
    "\n",
    "$$\n",
    "\\mathcal F = \\{f_{\\boldsymbol \\theta}(\\boldsymbol x) \\vert \\boldsymbol \\theta \\in \\mathbb R^n\\}\n",
    "$$\n",
    "\n",
    "which is also called a **model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e856d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To **fit** a model means to find $\\boldsymbol \\theta$ which minimizes the loss function\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol \\theta) = \\frac 1N \\sum\\limits_{i=1}^N \\ell(y_i, f_{\\boldsymbol \\theta}(\\boldsymbol x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b754d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7dc16",
   "metadata": {},
   "source": [
    "```{image} https://miro.medium.com/max/1400/1*biZq-ihFzq1I6Ssjz7UtdA.jpeg\n",
    ":alt: cats-vs-dogs\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e4748",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Binary classification**\n",
    "\n",
    "* $\\mathcal Y = \\{0, 1\\}$ or $\\mathcal Y = \\{-1, +1\\}$\n",
    "* denote model predictions as $\\hat y_i = f_{\\boldsymbol \\theta}(\\boldsymbol x_i)$\n",
    "* typical loss function is **misclassification rate**\n",
    "\n",
    "    ```{math}\n",
    "        :label: mis-rate\n",
    "        \\mathcal L(\\boldsymbol \\theta) = \\frac 1N \\sum\\limits_{i=1}^N \\big[y_i \\ne \\hat y_i\\big]\n",
    "    ```\n",
    "\n",
    "    (it actually equals one minus accuracy)\n",
    "\n",
    "* this loss is not a smooth function, that's why they often predict  which is treated as probability of class $1$, and then use **cross-entropy loss**\n",
    "\n",
    "```{math}\n",
    "        :label: binary-cross-entropy\n",
    "    \\mathcal L(\\boldsymbol \\theta) = -\\frac 1N \\sum\\limits_{i=1}^N \\big(y_i \\log(\\hat y_i) + (1-y_i) \\log(1 - \\hat y_i)\\big)\n",
    "```\n",
    "\n",
    "```{important}\n",
    "The value $0\\log 0 = 0$ by definition\n",
    "\n",
    "```\n",
    "\n",
    "````{admonition} Example\n",
    "Suppose that true labels $y$ and predictions $\\hat y$ are as follows:\n",
    "\n",
    "```{table} Binary classificaton\n",
    ":name: binary-metrics\n",
    "\n",
    "|$y$ | $\\hat y$|\n",
    "|:---:|:------:|\n",
    "|$0$| $0$  |\n",
    "|$0$| $1$  |\n",
    "|$1$| $0$  |\n",
    "|$1$| $1$  |\n",
    "|$0$| $0$  |\n",
    "```\n",
    "\n",
    "Calculate the missclassification rate and cross-entropy loss.\n",
    "````\n",
    "\n",
    "<span style=\"display:none\" id=\"binary_loss\">W3sicXVlc3Rpb24iOiAiQ2FsY3VsYXRlIHRoZSBtaXNjbGFzc2lmaWNhdGlvbiByYXRlIiwgInR5cGUiOiAibnVtZXJpYyIsICJhbnN3ZXJzIjogW3sidHlwZSI6ICJ2YWx1ZSIsICJ2YWx1ZSI6IDAuNCwgImNvcnJlY3QiOiB0cnVlLCAiZmVlZGJhY2siOiAiUmlnaHQgeW91IGFyZSEgSGVyZSAkTj01JCwgdHdvIGluY29ycmVjdCBwcmVkaWN0aW9ucywgc28gdGhlIGxvc3MgaXMgJFxcZnJhYyAyNSQifSwgeyJ0eXBlIjogImRlZmF1bHQiLCAiZmVlZGJhY2siOiAiTm9wZSJ9XX0sIHsicXVlc3Rpb24iOiAiQ2FsY3VsYXRlIHRoZSBjcm9zcy1lbnRyb3B5IGxvc3MiLCAidHlwZSI6ICJtYW55X2Nob2ljZSIsICJhbnN3ZXJzIjogW3siYW5zd2VyIjogIiQwJCIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJUb28gbG93In0sIHsiYW5zd2VyIjogIiRcXGZyYWMgMTUgXFxsb2cgMiQiLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRG9lc24ndCBsb29rIGNvcnJlY3QifSwgeyJhbnN3ZXIiOiAiJFxcZnJhYyAxNSBcXGxvZyAzJCIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJOb3BlIn0sIHsiYW5zd2VyIjogIiQrXFxpbmZ0eSQiLCAiY29ycmVjdCI6IHRydWUsICJmZWVkYmFjayI6ICJPaCwgeWVhaC4uLiJ9XX1d</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925cc6b1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jupyterquiz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyterquiz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_quiz\n\u001b[1;32m      2\u001b[0m display_quiz(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#binary_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jupyterquiz'"
     ]
    }
   ],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"#binary_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ad66b",
   "metadata": {},
   "source": [
    "To avoid such problems with loss {eq}`binary-cross-entropy` models usually predict numbers from $(0, 1)$, which are interpreted as probabilities of class $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26db24",
   "metadata": {},
   "source": [
    "**Multiclass classification**\n",
    "\n",
    "```{image} https://miro.medium.com/max/1400/1*JAXmOAImcf683aXaBDPPVg.jpeg\n",
    ":alt: multiclass\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593cad82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* $\\mathcal Y = \\{1, 2, \\ldots, K\\}$ \n",
    "* one-hot encoding: $\\boldsymbol y_i \\in \\{0, 1\\}^K$, $\\sum\\limits_{k=1}^K y_{ik} = 1$\n",
    "* $\\hat{\\boldsymbol y}_i = f_{\\boldsymbol \\theta}(\\boldsymbol x_i) \\in [0, 1]^K$ is now the vector of probabilities of belonging to class $k$: \n",
    "\n",
    "    $$\n",
    "        \\hat y_{ik} = \\mathbb P(\\boldsymbol x_i \\in \\text{ class }k)\n",
    "    $$\n",
    "    \n",
    "* the cross-entropy loss is now written as follows:\n",
    "\n",
    "```{math}\n",
    ":label: cross-entropy\n",
    "\\mathcal L(\\boldsymbol \\theta) = -\\frac 1N \\sum\\limits_{i=1}^N \\sum\\limits_{k=1}^Ky_{ik} \\log(\\hat y_{ik})\n",
    "```\n",
    "\n",
    "```{admonition} Example\n",
    "Classifying into $3$ classes, model produces the following outputs:\n",
    "\n",
    "|$y$ | $\\boldsymbol {\\hat y}$|\n",
    "|:---:|:-------------------:|\n",
    "|$0$| $(0.25, 0.4, 0.35)$  |\n",
    "|$0$| $(0.5, 0.3, 0.2)$  |\n",
    "|$1$| $\\big(\\frac 12 - \\frac 1{2\\sqrt 2}, \\frac 1{\\sqrt 2}, \\frac 12 - \\frac 1{2\\sqrt 2}\\big)$  |\n",
    "|$2$| $(0, 0, 1)$  |\n",
    "\n",
    "Calculate the cross-entropy loss {eq}`cross-entropy`. Assume that log base is $2$.\n",
    "```\n",
    "<span style=\"display:none\" id=\"cross_entropy_loss\">W3sicXVlc3Rpb24iOiAiQ2FsY3VsYXRlIHRoZSBjcm9zcyBlbnRyb3B5IGZyb20gdGhlIHByZXZpb3VzIGV4YW1wbGUiLCAidHlwZSI6ICJudW1lcmljIiwgImFuc3dlcnMiOiBbeyJ0eXBlIjogInZhbHVlIiwgInZhbHVlIjogMC44NzUsICJjb3JyZWN0IjogdHJ1ZSwgImZlZWRiYWNrIjogIkV4YWN0bHkhIn0sIHsidHlwZSI6ICJkZWZhdWx0IiwgImZlZWRiYWNrIjogIkluY29ycmVjdCJ9XX1d</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e75d8a",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_quiz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay_quiz\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#cross_entropy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_quiz' is not defined"
     ]
    }
   ],
   "source": [
    "display_quiz(\"#cross_entropy_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7344a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad99c8",
   "metadata": {},
   "source": [
    "* $\\mathcal Y = \\mathbb R$ or $\\mathcal Y = \\mathbb R^n$\n",
    "* the common choice is the quadratic loss \n",
    "\n",
    "    $$\n",
    "        \\ell_2(y, \\hat y) = (y - \\hat y)^2\n",
    "    $$\n",
    "* then the overall loss function — mean squared error:\n",
    "\n",
    "    $$\n",
    "    \\mathcal L(\\boldsymbol \\theta) = \\mathrm{MSE}(\\boldsymbol \\theta) = \\frac 1N\\sum\\limits_{i=1}^N (y_i - f_{\\boldsymbol \\theta}(\\boldsymbol x_i))^2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4453d8",
   "metadata": {},
   "source": [
    "If the function $f_{\\boldsymbol \\theta}(\\boldsymbol x_i) = \\boldsymbol {\\theta^\\top x}_i + b$ is linear, then the model is called **linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d1bdc",
   "metadata": {},
   "source": [
    "Example of one-dimensional linear regression (figure 1.5 from {cite:p}`pml1Book`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf96bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccf3023",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "N = 21\n",
    "x = np.linspace(0.0, 20, N)\n",
    "X0 = x.reshape(N, 1)\n",
    "X = np.c_[np.ones((N, 1)), X0]\n",
    "w = np.array([-1.5, 1 / 9.0])\n",
    "y = w[0] * x + w[1] * np.square(x)\n",
    "y = y + np.random.normal(0, 1, N) * 2\n",
    "\n",
    "w = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "# print(w)\n",
    "y_estim = np.dot(X, w)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(X[:, 1], y, \"o\")\n",
    "ax1.plot(X[:, 1], y_estim, \"-\")\n",
    "\n",
    "for x0, y0, y_hat in zip(X[:, 1], y, y_estim):\n",
    "    ax2.plot([x0, x0], [y0, y_hat], \"k-\")\n",
    "ax2.plot(X[:, 1], y, \"o\")\n",
    "ax2.plot(X[:, 1], y_estim, \"-\")\n",
    "ax2.plot(X[:, 1], y_estim, \"x\", color=\"r\", markersize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bce9e",
   "metadata": {},
   "source": [
    "**Q.** Suppose that training dataset has only one sample ($N=1$) and one feature ($n=1$). How would linear regression look like in this case? What if $N=2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed235c9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d427b0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import rc\n",
    "from scipy.special import expit\n",
    "\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex', preamble=r'\\usepackage[russian]{babel}')\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'size'   : 24,\n",
    "        'weight' : 'heavy'\n",
    "       }\n",
    "\n",
    "rc('font', **font)\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "def plot_sigmoid(xmin, xmax, ymin, ymax):\n",
    "    text_size = 24\n",
    "    legend_size = 20\n",
    "    eps=0.2\n",
    "    fig, ax = plt.subplots(figsize=(11, 6))\n",
    "    xs = np.linspace(xmin, xmax, num=500)\n",
    "    \n",
    "    \n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "\n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "\n",
    "    ax.text(xmax + eps, -.2, r\"$x$\", size=text_size)\n",
    "    ax.text(0.1, ymax, r\"$y$\", size=text_size)\n",
    "    \n",
    "    arrow_fmt = dict(markersize=6, color='black', clip_on=False)\n",
    "    ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "    ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "    \n",
    "    ax.plot(xs, expit(xs), c='r', lw=3, label= r'$\\sigma(x) = \\frac{1}{1+e^{-x}}$')\n",
    "    # plt.plot(xs, np.maximum(0.2*xs, xs), c='m', lw=3, label= r'$\\mathrm{LReLU}(x)$')\n",
    "    \n",
    "    ax.plot([0, xmax], [1, 1], c='k', ls='--', lw=2)\n",
    "    ax.plot([xmin, 0], [-1, -1], c='k', ls='--', lw=2)\n",
    "    \n",
    "    ax.text(-0.18, 0.05, r\"0\")\n",
    "    \n",
    "    ax.legend(fontsize=legend_size);\n",
    "    ax.grid(ls=':')\n",
    "    ax.set_xlim(xmin-eps, xmax+eps)\n",
    "    ax.set_ylim(ymin - eps/2, ymax+eps/2)\n",
    "    yticks = np.arange(ymin, ymax+1)\n",
    "    xticks = np.arange(xmin, xmax+1)\n",
    "    ax.set_yticks(yticks[yticks != 0]);\n",
    "    ax.set_xticks(xticks[xticks != 0])\n",
    "    ax.set_yticklabels(yticks[yticks != 0], size=legend_size)\n",
    "    ax.set_xticklabels(xticks[xticks != 0], size=legend_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a43aff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1440/1*YUl_BcqFPgX49sSb5yrk3A.jpeg\n",
    ":alt: unsupervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922a8c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No targets anymore! The training dataset $\\mathcal D = (\\boldsymbol x_i)_{i=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ff6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of unsupervised learning tasks:\n",
    "* clustering\n",
    "* dimension reduction\n",
    "* discovering latent factors\n",
    "* searching for association rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c48b5d",
   "metadata": {},
   "source": [
    "Clusterisation made on {ref}`iris` (figure 1.8 from {cite:p}`pml1Book`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e867aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec08b5d1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "K = 3\n",
    "idx1 = 2\n",
    "idx2 = 3\n",
    "y_pred = GaussianMixture(n_components=K, random_state=42).fit(X).predict(X)\n",
    "mapping = np.array([2, 0, 1])\n",
    "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])\n",
    "\n",
    "colors = sns.color_palette()[0:K]\n",
    "markers = (\"s\", \"x\", \"o\", \"^\", \"v\")\n",
    "fig, ax = plt.subplots()\n",
    "for k in range(0, K):\n",
    "    ax.plot(\n",
    "        X[y_pred == k, idx1],\n",
    "        X[y_pred == k, idx2],\n",
    "        color=colors[k],\n",
    "        marker=markers[k],\n",
    "        linestyle=\"None\",\n",
    "        label=\"Cluster {}\".format(k),\n",
    "    )\n",
    "ax.set(xlabel=iris.feature_names[idx1])\n",
    "ax.set(ylabel=iris.feature_names[idx2])\n",
    "plt.legend(loc=\"upper left\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e92206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semisupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a071",
   "metadata": {},
   "source": [
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1600/1*0TUC4m6yB7HUuPNO2SXEBw.png\n",
    ":alt: semisupervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Semi-supervised learning comes into play when you have a dataset that contains both labeled and unlabeled data. Semi-supervised learning is often used in scenarios where obtaining labeled data is expensive, time-consuming, or otherwise challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb47bc",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "**Reinforcement learning** is a machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment. It aims to maximize a cumulative reward signal by exploring actions and learning optimal strategies through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd4b45",
   "metadata": {},
   "source": [
    "```{admonition} TODO\n",
    ":class: warning\n",
    "* Pictures from the internet is a temporary solution, try to create original ones\n",
    "* Add a subsection about dummy model (move something from the next chapter if necessary)\n",
    "* Write more about ML beyond supervised learning\n",
    "* Convert $N$ and $D$ into $n$ and $d$\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}